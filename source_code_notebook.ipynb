{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OIL & GAS PRODUCTION - Text Mining Accident Reports \n",
    "\n",
    "### This project analyzes worker accidents based on the summary reports provided by Occupational Safety and Health Administration (OSHA) and classified according to NAICS system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "The following are the steps required in performing the necessary analysis on the textual corpus of accident reports. \n",
    "Each of these steps further contain multiple tasks which are discussed in detail under their respective sections.\n",
    "### 1. Data Acquisition\n",
    "### 2. Text Preprocessing \n",
    "### 3. Text Structuring\n",
    "### 4. Text Mining \n",
    "### 5. Advanced Analytics\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition \n",
    "The datasets are obtained from \"osha.gov\" website. Each accident report contains the **Summary** along with **Report ID**, **Event Date** and **Establishment Name**.\n",
    "As the report is not available in a downloadable format, they will be scraped from the website for a selected timeline. \n",
    "\n",
    "For this demo, text from a single report is used to show the different steps involved in the process. For the project though, text from all reports will be downloaded into a single csv file based on the **Report ID** as primary key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data into Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 8:37 a.m. on January 22, 2018, five employees were working on an oil rig, tripping out the wellbore to change a bit. During work, the gas surfaced as the pipe/bit was removed from the well, causing the well to burp. Once the initial burp subsided, another burp occurred and gas fumes found an ignition source, causing a series of explosions on and around the rig. Five employees were in the doghouse, unable to escape and were over come by smoke and excessive heat. Three employees were killed.\n"
     ]
    }
   ],
   "source": [
    "raw_rpt = \"At 8:37 a.m. on January 22, 2018, five employees were working on an oil rig, tripping out the wellbore to change a bit. During work, the gas surfaced as the pipe/bit was removed from the well, causing the well to burp. Once the initial burp subsided, another burp occurred and gas fumes found an ignition source, causing a series of explosions on and around the rig. Five employees were in the doghouse, unable to escape and were over come by smoke and excessive heat. Three employees were killed.\"\n",
    "print(raw_rpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text from a random report is assigned to a string variable in Python. We can look at some basic features like length of the report by line count and word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 5\n",
      "Number of words: 88\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of lines:\", raw_rpt.count('. '))\n",
    "print(\"Number of words:\", raw_rpt.count(' ')+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a large corpus of data, as we cannot browse at individual documents it is useful to have a look at some of these features to know if some of files have no data or if some files contain corrupted text in abnormal format with special characters and so on.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "Preprocessing is an important yet time-consuming process in analyzing textual data. It is vital because we need to convert human readable text into machine readable format which involves several steps with multiple tools or packages. Rather than importing all packages at the beginning, for now I will import each of them where they are used in their respective cells.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "We will start with Text Normalization which includes:\n",
    "- **converting all letters to lower or upper case**\n",
    "- **converting numbers into words or removing numbers**\n",
    "- **removing punctuations, accent marks and other diacritics**\n",
    "- **removing white spaces**\n",
    "- **expanding abbreviations**\n",
    "- **removing stop words, sparse terms, and particular words**\n",
    "\n",
    "Depending on our needs and nature of the data some of these steps might be redundant. We can remove or add some steps as we seem fit to not lose any important details in the data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at 8:37 a.m. on january 22, 2018, five employees were working on an oil rig, tripping out the wellbore to change a bit. during work, the gas surfaced as the pipe/bit was removed from the well, causing the well to burp. once the initial burp subsided, another burp occurred and gas fumes found an ignition source, causing a series of explosions on and around the rig. five employees were in the doghouse, unable to escape and were over come by smoke and excessive heat. three employees were killed.\n"
     ]
    }
   ],
   "source": [
    "report = raw_rpt.lower()\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers\n",
    "Generally numbers are not relevant for text analysis as it can be more difficult to extract context or meaning from numbers than from words. In our report, we can observe that the numbers used indicate time and date which is already provided in a separate field if we need them. The last line in the report contains vital information that \"three employees are killed\" in word form, thus we are not discarding any useful data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at : a.m. on january , , five employees were working on an oil rig, tripping out the wellbore to change a bit. during work, the gas surfaced as the pipe/bit was removed from the well, causing the well to burp. once the initial burp subsided, another burp occurred and gas fumes found an ignition source, causing a series of explosions on and around the rig. five employees were in the doghouse, unable to escape and were over come by smoke and excessive heat. three employees were killed.\n"
     ]
    }
   ],
   "source": [
    "import re  # import Regex library \n",
    "report = re.sub(\"\\d+\", \"\", report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation\n",
    "We can use the `string` library to remove any symbols like [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at  am on january   five employees were working on an oil rig tripping out the wellbore to change a bit during work the gas surfaced as the pipebit was removed from the well causing the well to burp once the initial burp subsided another burp occurred and gas fumes found an ignition source causing a series of explosions on and around the rig five employees were in the doghouse unable to escape and were over come by smoke and excessive heat three employees were killed\n"
     ]
    }
   ],
   "source": [
    "import string  # Along with importing string, we are also using the 're' Regex library\n",
    "report = re.sub('[%s]' % re.escape(string.punctuation), '', report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove whitespaces\n",
    "We will use the `strip()` method to remove ant leading or trailing whitespaces and `re.sub` method from Regex to remove any duplicate whitespaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at am on january five employees were working on an oil rig tripping out the wellbore to change a bit during work the gas surfaced as the pipebit was removed from the well causing the well to burp once the initial burp subsided another burp occurred and gas fumes found an ignition source causing a series of explosions on and around the rig five employees were in the doghouse unable to escape and were over come by smoke and excessive heat three employees were killed\n"
     ]
    }
   ],
   "source": [
    "report = report.strip()\n",
    "report = re.sub(' +', ' ',report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and remove stopwords\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Stop words (or commonly occurring words) should be removed from the text data as they do not provide any value and also makes our word matrix much sparse later. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries. Here we'll use `nltk` library which contains all basic stopwords in English language. \n",
    "Apart from these words we can also remove some specific words by creating a word set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'am', 'on', 'january', 'five', 'employees', 'were', 'working', 'on', 'an', 'oil', 'rig', 'tripping', 'out', 'the', 'wellbore', 'to', 'change', 'a', 'bit', 'during', 'work', 'the', 'gas', 'surfaced', 'as', 'the', 'pipebit', 'was', 'removed', 'from', 'the', 'well', 'causing', 'the', 'well', 'to', 'burp', 'once', 'the', 'initial', 'burp', 'subsided', 'another', 'burp', 'occurred', 'and', 'gas', 'fumes', 'found', 'an', 'ignition', 'source', 'causing', 'a', 'series', 'of', 'explosions', 'on', 'and', 'around', 'the', 'rig', 'five', 'employees', 'were', 'in', 'the', 'doghouse', 'unable', 'to', 'escape', 'and', 'were', 'over', 'come', 'by', 'smoke', 'and', 'excessive', 'heat', 'three', 'employees', 'were', 'killed']\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "\"\"\"\n",
    "If python gives out error for not finding some subpackages, \n",
    "running these code lines may help: \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') \n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenize\n",
    "tokens = word_tokenize(report)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['january', 'five', 'employees', 'working', 'oil', 'rig', 'tripping', 'wellbore', 'change', 'bit', 'work', 'gas', 'surfaced', 'pipebit', 'removed', 'well', 'causing', 'well', 'burp', 'initial', 'burp', 'subsided', 'another', 'burp', 'occurred', 'gas', 'fumes', 'found', 'ignition', 'source', 'causing', 'series', 'explosions', 'around', 'rig', 'five', 'employees', 'doghouse', 'unable', 'escape', 'come', 'smoke', 'excessive', 'heat', 'three', 'employees', 'killed']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wrd_tkns = [i for i in tokens if not i in stop_words]\n",
    "print (wrd_tkns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the string is now broken into a list of strings with words. When we have multiple reports the words will we be seperated into columns in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are **Porter** stemming algorithm (removes common morphological and inflexional endings from words) and **Lancaster** stemming algorithm (a more aggressive stemming algorithm). \n",
    "\n",
    "We will try both Porter and Lancaster Stemmers again from `nltk` library and see which of these produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['januari', 'five', 'employe', 'work', 'oil', 'rig', 'trip', 'wellbor', 'chang', 'bit', 'work', 'ga', 'surfac', 'pipebit', 'remov', 'well', 'caus', 'well', 'burp', 'initi', 'burp', 'subsid', 'anoth', 'burp', 'occur', 'ga', 'fume', 'found', 'ignit', 'sourc', 'caus', 'seri', 'explos', 'around', 'rig', 'five', 'employe', 'doghous', 'unabl', 'escap', 'come', 'smoke', 'excess', 'heat', 'three', 'employe', 'kill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "stm_tkns = [stemmer.stem(word) for word in wrd_tkns]\n",
    "print(stm_tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['janu', 'fiv', 'employ', 'work', 'oil', 'rig', 'trip', 'wellb', 'chang', 'bit', 'work', 'gas', 'surfac', 'pipebit', 'remov', 'wel', 'caus', 'wel', 'burp', 'init', 'burp', 'subsid', 'anoth', 'burp', 'occur', 'gas', 'fum', 'found', 'ignit', 'sourc', 'caus', 'sery', 'explod', 'around', 'rig', 'fiv', 'employ', 'dogh', 'un', 'escap', 'com', 'smok', 'excess', 'heat', 'three', 'employ', 'kil']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer= LancasterStemmer()\n",
    "stm_tkns = [stemmer.stem(word) for word in wrd_tkns]\n",
    "print(stm_tkns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the resulting words from both the stemmers are not quite satisfactory. For many of the words the last characters are discarded and the resulting words are not meaningful and neither actual words of English language.\n",
    "Hence we will try lemmatization, which is a similar process to stemming. But for lemmatization to work effectively we will need the part of speech of the words. So we will first tag our words with their parts of speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (POS) Tagging\n",
    "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('january', 'JJ'), ('five', 'CD'), ('employees', 'NNS'), ('working', 'VBG'), ('oil', 'NN'), ('rig', 'NN'), ('tripping', 'VBG'), ('wellbore', 'JJR'), ('change', 'NN'), ('bit', 'NN'), ('work', 'NN'), ('gas', 'NN'), ('surfaced', 'VBD'), ('pipebit', 'NN'), ('removed', 'VBN'), ('well', 'RB'), ('causing', 'VBG'), ('well', 'RB'), ('burp', 'RB'), ('initial', 'JJ'), ('burp', 'NN'), ('subsided', 'VBD'), ('another', 'DT'), ('burp', 'NN'), ('occurred', 'VBD'), ('gas', 'NN'), ('fumes', 'NNS'), ('found', 'VBN'), ('ignition', 'NN'), ('source', 'NN'), ('causing', 'VBG'), ('series', 'NN'), ('explosions', 'NNS'), ('around', 'IN'), ('rig', 'NN'), ('five', 'CD'), ('employees', 'NNS'), ('doghouse', 'VBP'), ('unable', 'JJ'), ('escape', 'NN'), ('come', 'VB'), ('smoke', 'NN'), ('excessive', 'JJ'), ('heat', 'NN'), ('three', 'CD'), ('employees', 'NNS'), ('killed', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "# We will use the nltk package method to create tags \n",
    "pos_tags = nltk.pos_tag(wrd_tkns)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained POS tags for all of our words, but they are in what called as a Penn Treebank format. For the `nltk`'s  lemmatizer function to work we need the tags in a Wordnet format. Therefore I created a small mapper function to convert Penn tags to Wordnet tags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part is a dictionary containing the Penn to Wordnet mapping as key, value pairs\n",
    "part = {\n",
    "    'N' : 'n',\n",
    "    'V' : 'v',\n",
    "    'J' : 'a',\n",
    "    'S' : 's',\n",
    "    'R' : 'r'\n",
    "}\n",
    "\n",
    "\n",
    "def convert_tag(penn_tag):\n",
    "    \"\"\"\n",
    "    convert_tag() accepts the **first letter** of a Penn part-of-speech tag,\n",
    "    then uses a dict lookup to convert it to the appropriate WordNet tag.\n",
    "    \"\"\"\n",
    "    if penn_tag in part.keys():\n",
    "        return part[penn_tag]\n",
    "    else:\n",
    "        # other parts of speech will be tagged as nouns\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('january', 'a'), ('five', 'n'), ('employees', 'n'), ('working', 'v'), ('oil', 'n'), ('rig', 'n'), ('tripping', 'v'), ('wellbore', 'a'), ('change', 'n'), ('bit', 'n'), ('work', 'n'), ('gas', 'n'), ('surfaced', 'v'), ('pipebit', 'n'), ('removed', 'v'), ('well', 'r'), ('causing', 'v'), ('well', 'r'), ('burp', 'r'), ('initial', 'a'), ('burp', 'n'), ('subsided', 'v'), ('another', 'n'), ('burp', 'n'), ('occurred', 'v'), ('gas', 'n'), ('fumes', 'n'), ('found', 'v'), ('ignition', 'n'), ('source', 'n'), ('causing', 'v'), ('series', 'n'), ('explosions', 'n'), ('around', 'n'), ('rig', 'n'), ('five', 'n'), ('employees', 'n'), ('doghouse', 'v'), ('unable', 'a'), ('escape', 'n'), ('come', 'v'), ('smoke', 'n'), ('excessive', 'a'), ('heat', 'n'), ('three', 'n'), ('employees', 'n'), ('killed', 'v')]\n"
     ]
    }
   ],
   "source": [
    "wrdnet_tags = [(word, convert_tag(tag[0])) for word, tag in pos_tags]\n",
    "print(wrdnet_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The POS tags are now converted into Wordnet format and we can input these now to the lemmatizer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Astron\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download if 'wordnet' not in nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['january', 'five', 'employee', 'work', 'oil', 'rig', 'trip', 'wellbore', 'change', 'bit', 'work', 'gas', 'surface', 'pipebit', 'remove', 'well', 'cause', 'well', 'burp', 'initial', 'burp', 'subside', 'another', 'burp', 'occur', 'gas', 'fume', 'find', 'ignition', 'source', 'cause', 'series', 'explosion', 'around', 'rig', 'five', 'employee', 'doghouse', 'unable', 'escape', 'come', 'smoke', 'excessive', 'heat', 'three', 'employee', 'kill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "lmt_tkns = [lemmatizer.lemmatize(word[0], word[1][0]) for word in wrdnet_tags]\n",
    "print(lmt_tkns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the words are now reduced at a sufficient level to their base form. So, when we import data of all reports in a dataframe we can skip the Stemming part and work with the lemmatizer to trim the words. \n",
    "\n",
    "This concludes text preprocessing to some extent. These are majority of the steps involved in the process and a few more may be added as we scale up. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
